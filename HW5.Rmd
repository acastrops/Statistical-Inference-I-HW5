---
title: "Statistical Inference 1 - Homework 5"
author: "Adriana Souza, Lilly Raud, Kevin Hunt, Saad Usmani, Mike McCormack"
date: "11/22/2017"
output:
  html_document: default
  #pdf_document:
   # latex_engine: lualatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library('mosaic')
library('Hmisc')
```

# Guidelines

+ This is a group project. You may work in groups with up to 6 people. You only need to turn in one homework assignment for each group, but make sure that everyone's name is listed on the assignment. 
+ Also, even if you don't write the code for every part of the assignment, you should practice the skills in each section.
+ This assignment focuses on simulating data that violates various assumptions of the linear regression model.
+ Some sample code has been included, but you will need to try many different starting values for the parameters, sample size, and distributional assumptions. The included code is just to give you a starting point; it should not be considered sufficient to answer all the questions in each part. (And you are welcome to ignore the sample code and use your own)

\newpage

# Questions

1.	Nonlinear relationship
    a.	Simulate data for a variety of different non-linear relationships (e.g. polynomial, exponential, sinusoidal).
    b.	Try simulations with a small sample size (e.g. 20), a medium sample size (e.g. n= 100), and a large sample size (e.g. n = 5000). 
    c.	For each simulation, 
        i.	Predict y-hat at several different locations using a confidence interval.
        ii.	Predict the beta coefficients for a linear model using a confidence interval. 
        iii. Find the MSE (to estimate sigma^2)
        iv. Test to see whether the beta(s) are significant
    d. Which of the above tasks were affected by the nonlinear relationship?
    e. After you have experimented with the effects of different model structures, true parameter values, and sample sizes, let's repeat the simulation but test yourself to see whether you can detect non-linearity.
        i. Have R randomly choose whether to simulate data from a true linear model or a true nonlinear model.
        ii. Simulate data accordingly and display informal/ formal diagnostics as appropriate.
        iii. Based on the diagnostics, predict whether the problem areas you mentioned in part d will be affected or not. (Note: You are not predicting whether the assumptions are violated-- just whether they are violated to such an extent that your ability to use the model is compromised)

__Aside:__ Many of the issues you end up facing with a nonlinear relationship can also be seen if an important predictor is excluded from the model. If you have extra time, feel free to play with this issue as well (optional).


```{r}
# Sample code to get started
set.seed(1)

# creates information for a normal model to be used in every question part e
normal <- function(n, b0, b1, bool){
  x1 <- runif(n, 0, 10)
  norm.eps <- 3 * rnorm(n)
  norm.var <- var(norm.eps)
  norm.y <-b0 + b1*(x1) + norm.eps
  norm.ey <-b0 + b1*(x1)
  norm.lm <- lm(norm.y ~ x1)
  testing(norm.lm, norm.ey, list(b0,b1), norm.var, bool)
}


# a, c
Nonlinear <- function(n, func, bool = FALSE){

  # Melissa gave us
  x1 <- runif(n, 0, 10)
  eps <- 3 * rnorm(n)
  var <- var(eps)
  # y <- b0 - b1 * (x1 - 5)^2 + eps
  x2 <- runif(n, 0, 10)
  samp <- c("sinusoidal", "polynomial", "exponential", "normal")
  b0 <- 10
  b1 <- 2
  b2 <- 5
  
  # exponential
  if (func == "exponential"){
    exp.y <- b0 + b1*exp(x1) + eps
    exp.ey <- b0 + b1*exp(x1)
    exp.lm <- lm(exp.y ~ x1)
    testing(exp.lm, exp.ey, list(b0,b1), var, bool)
  }
  
  
  # polynomial
  if(func == "polynomial"){
    pol.y <- b0 - b1 * (x1 - 5)^2 + b2*x2 + eps
    pol.ey <- b0 - b1 * (x1 - 5)^2 + b2*x2
    pol.lm <- lm(pol.y ~ x1 + x2)
    testing(pol.lm, pol.ey, list(b0,b1, b2), var, bool)
  }
  
  # sinusoidal
  if(func == "sinusoidal"){
    sin.y <-b0 + b1*sin(x1) + eps
    sin.ey <-b0 + b1*sin(x1)
    sin.lm <- lm(sin.y ~ x1)
    testing(sin.lm, sin.ey, list(b0,b1), var, bool)
  }
  
  # normal
  if(func == "normal"){
    normal(n, b0, b1, bool)
  }
}

# a function to call the other methods
testing <- function(lm, ey, betaslist, var, bool){
  stats <- reg.stats(lm)
  tests <- reg.tests(stats[1], stats[2], stats[4], ey, betaslist)
  output(tests[1], tests[2], tests[3], stats[3], var, lm, bool)
}

# calculates the yhat, confint for the betas, the mse, grabs the p-values for the coefficients and returns
reg.stats <- function(lm){
  yhat <- predict(lm, interval = "confidence")
  confint <- confint(lm)
  mse <- anova(lm)[2,3]
  pvalue <- summary(lm)$coefficients[,4]
  return(list(yhat, confint, mse, pvalue))
}

# d
# creates the tests of EYi and beta coverage in the confidence intervals and if we corrected concluded b != 0
reg.tests <- function(yhat, confint, pvalue, ey, betaslist)  {

  # create a percentage of EYi's that fall within the confidence interval of Yhat
  yhat.test <- vector(mode="logical", length = length(ey))
  for (i in 1:length(ey)){
    yhat.test[i] <- (ey[i] >= yhat[[1]][i, 2] & ey[i] <= yhat[[1]][i, 3])
  }
  yhat.coverage <- sum(yhat.test, na.rm=TRUE)/length(ey)*100
  
  # beta coverage
  # creates a boolean of if all beta's fall within their confidence intervals
  beta.test <- vector(mode="logical", length = length(betaslist))
  for (b in 1:length(betaslist)){
    beta.test[b] <- (betaslist[b] >= confint[[1]][b,1] & betaslist[b] <= confint[[1]][b, 2])
  }
  beta.pass <- all(beta.test)
  
  # pvalue test
  # create a data frame with coefficient name and boolean of if pvalue significant
  coefficient.df <- data.frame(matrix(ncol = 2, nrow = length(betaslist)))
  for (p in 1:length(pvalue[[1]])){
    coefficient.df[p, 1] <-names(pvalue[[1]])[p]
    coefficient.df[p, 2] <- (pvalue[[1]][p] <= 0.05)
  }
  return(list(yhat.coverage, beta.pass, coefficient.df))
}

# puts all the information together into an output
output <- function(yhat.coverage, beta.pass, coefficient.df, mse, var, lm, bool){
  # is beta in confint?
  print(paste("Betas are within their confidence interval: ", beta.pass[[1]]))

  
  # is EYi in confint for Yhat?
  print(paste("How often does the EYi fall within the Yhat confidence interval?: ", yhat.coverage, "% of the time"))
  
  # How close is MSE to sigma^2? Later this will be represented as a histogram for multiple simulations
  print(paste("MSE is: ", mse, "vs. the variance: ", var))
  
  # Did we correctly conclude beta does not equal 0?
  if (!all(coefficient.df[[1]]$X2)){
    print(paste("The following coefficients were not significant: ", coefficient.df[[1]]$X1[coefficient.df[[1]]$X2 == FALSE]))
    print("")
  }
  else {
    print("All coefficients are significant")
    print("")
  }
  
  if (bool){
    plot(lm)
  }
}
# b
# get information about each case
size <- c(20,100,5000)
samp <- c("sinusoidal", "polynomial", "exponential", "normal")
for (i in 1:length(samp)){
  for (j in 1:length(size)){
    print(paste("Running sample ", samp[i]," with size ",size[j]))
    Nonlinear(size[j], samp[i])
  }
}
```
Effect of nonlinearity on each simulation.  

Sinusoidal  
    Small  
    - Betas do not fall with in the confidence interval  
    - Questionable EYi prediction of (70%)  
    - Discrepency between MSE and variance is small.  
    - The coefficient for x1 is not significant.
  
  Medium  
    - Betas do not fall with in the confidence interval  
    - Poo EYi prediction of (35%)  
    - Discrepency between MSE and variance is very small.  
    - The coefficient for x1 is not significant.
  
  Large  
    - Betas do not fall with in the confidence interval  
    - Horrible EYi prediction of (5.88%)  
    - Discrepency between MSE and variance is small.  
    - The coefficient for x1 is not significant.
  
Polynomial  
    Small  
    - Betas do fall with in the confidence interval  
    - Poor EYi prediction of (45%)  
    - Discrepency between MSE and variance.  Not has large as the exponential difference for the small sample size.  
    - The coefficient for the intercept and x1 is not significant
  
  Medium  
    - Betas do not fall with in the confidence interval  
    - Bad EYi prediction of (21%)  
    - Discrepency between MSE and variance.  Not has large as the exponential difference for the medium sample size.  
    - The coefficient for x1 is not significant
  
  Large  
    - Betas do not fall with in the confidence interval  
    - Horrid EYi prediction of (2.72%)  
    - Discrepency between MSE and variance.  Not has large as the exponential difference for the large sample size.  
    - The coefficient for x1 is not siginficant
  
Exponential  
    Small  
      - Betas do not fall with in the confidence interval  
      - Questionable EYi prediction of (50%)  
      - MSE vs variance has a huge difference  
      - All coefficients are significant
  
  Medium  
      - Betas do not fall with in the confidence interval  
      - Horrible EYi prediction of (13%)  
      - MSE vs variance has an even larger difference than the small exponential function  
      - All coefficients are significant  
    
  Large  
    - Betas do not fall with in the confidence interval  
    - Atrocious EYi prediction of (2.78%)  
    - The discrepency between MSE vs variance is the worst out of all the exponential functions  
    - All coefficients are significant.
  
```{r}
set.seed(4)
# choose random sample size and random function
# Repeat to your hearts desire and test yourself
n <- sample(20:5000, 1)
i <- sample(1:4, 1)
Nonlinear(n, samp[i], TRUE)

```

  This seems to be an example of a Large Sinusoidal relationship.  This is determined by comparing the diagnostics of this simulation to the diagnostics that we calculated.  Here we can see that x1 coefficient is not significant and EYi falls within the Yhat confidence interval about only 8% of the time.  Comparing this to our other simulations, we see that the diagnostics here most closely resemble the diagnostics of a large sinusoidal simulation.




\newpage

2.	Non-normal errors
    a.	Simulate errors from a variety of different non-normal distributions (e.g. gamma, poisson). Make sure to shift the errors over so that they are still centered at 0.
    b.	Try simulations with a small sample size (e.g. 20), a medium sample size (e.g. n= 100), and a large sample size (e.g. n = 5000). 
    c.	For each simulation, 
        i.	Predict y-hat at several different locations using a confidence interval.
        ii.	Predict the beta coefficients for a linear model using a confidence interval. 
        iii. Find the MSE (to estimate sigma^2)
        iv. Test to see whether the beta(s) are significant (t-tests)
    d. Which of the above tasks were affected by the violation of assumptions?
    e. After you have experimented with the effects of different model structures, true parameter values, and sample sizes, let's repeat the simulation but test yourself to see whether you can detect non-normality.
        i. Have R randomly choose whether to simulate data with normal or nonnormal errors
        ii. Simulate data accordingly and display informal/ formal diagnostics as appropriate.
        iii. Based on the diagnostics, predict whether the problem areas you mentioned in part d will be affected or not. (Note: You are not predicting whether the assumptions are violated-- just whether they are violated to such an extent that your ability to use the model is compromised)

```{r}
set.seed(2)
nonnormal_functions <- c("poisson","gamma", "normal")

Nonnormal <- function(n, func, bool = FALSE){
  b0 <- 10
  b1 <- 2
  eps_alpha <- 2 #Shape
  eps_beta <- 1/4 #Scale
  
  x <- runif(n, 0, 10)

  # poisson
  if (func == "poisson"){
    eps <- rpois(n, lambda = 1)
    var <- var(eps)
    pois.y <- b0 - b1*(x) + eps
    pois.ey <- b0 - b1*(x)
    pois.lm <- lm(pois.y ~ x)
    testing(pois.lm, pois.ey, list(b0,b1), var, bool)
  }
  
  if(func == "gamma"){
    eps <- (rgamma(n, eps_alpha, 1/eps_beta) - eps_alpha * eps_beta) * 2
    var <- var(eps)
    gamma.y <- b0 - b1*(x) + eps
    gamma.ey <- b0 - b1*(x)
    gamma.lm <- lm(gamma.y ~ x)
    testing(gamma.lm, gamma.ey, list(b0,b1), var, bool)
  }
  
  # normal
  if(func == "normal"){
    normal(n, b0, b1, bool)
  }
  
}

size <- c(20,100,5000)
for (i in 1:length(nonnormal_functions)){
  for (j in 1:length(size)){
    print(paste("Running sample ", nonnormal_functions[i]," with size ",size[j]))
    Nonnormal(size[j], nonnormal_functions[i])
  }
}
```
Effect of non-normal errors on each simulation.  

Poisson Distribution  
    Small 
      - Betas are not within the confidence interval  
      - Low EYi predition of (25%)  
      - MSE and variance are very close together  
      - All coefficients are significant.   

  Medium  
    - Betas are not within the confidence interval  
    - Worst possible EYi predictions (0%)  
    - MSE and variance are very close together  
    - All coefficients are significant.
    
  Large  
    - Betas are not within the confidence interval  
    - Worst possible EYi predictions (0%)  
    - MSE and variance are almost identical  
    - All coefficients are significant
    
Gamma Distribution  
    Small  
      - Betas are not within the confidence interval  
      - Perfect EYi predictions (100%)  
      - MSE and variance are very close together  
      - All coefficients are significant  
      
  Medium
    - Betas are not within the confidence interval 
    - Perfect EYi predictions (100%)  
    - MSE and variance are very close together  
    - All coefficients are significant  
    
  Large  
    - Betas are not within the confidence interval  
    - Perfect EYi predictions (100%)  
    - MSE and variance are almost identical  
    - All coefficients are significant  
```{r}
set.seed(6)
# choose random sample size and random function
# Repeat to your hearts desire and test yourself (Remove seed to get different values.  Seed is set so we can show the model that we're interpreting)
n <- sample(20:5000, 1)
i <- sample(1:3, 1)
Nonnormal(n,nonnormal_functions[i],TRUE)

```


  This seems to be an example of a Large Normal distribution.  This is determined by comparing the diagnostics of this simulation to the diagnostics that we calculated.  Here we can see that all coefficients are significant and EYi falls within the Yhat confidence interval 100% of the time.  Comparing this to our other simulations and looking at the Q-Q plot for this model, we see that the diagnostics here most closely resemble the diagnostics of a large normal distribution


\newpage
        
3. Heterogeneous Variances
    a.	Simulate errors from a variety of different relationships with X (e.g. eps = 2 * sqrt(x))
    b.	Try simulations with a small sample size (e.g. 20), a medium sample size (e.g. n= 100), and a large sample size (e.g. n = 5000). 
    c.	For each simulation, 
        i.	Predict y-hat at several different locations using a confidence interval.
        ii.	Predict the beta coefficients for a linear model using a confidence interval. 
        iii. Find the MSE (to estimate sigma^2-- does that even make sense here?)
        iv. Test to see whether the beta(s) are significant (t-tests)
    d. Which of the above tasks were affected by the violation of assumptions?
    e. After you have experimented with the effects of different model structures, true parameter values, and sample sizes, let's repeat the simulation but test yourself to see whether you can detect heteroskedacity.
        i. Have R randomly choose whether to simulate errors with constant or non-constant variance
        ii. Simulate data accordingly and display informal/ formal diagnostics as appropriate.
        iii. Based on the diagnostics, predict whether the problem areas you mentioned in part d will be affected or not. (Note: You are not predicting whether the assumptions are violated-- just whether they are violated to such an extent that your ability to use the model is compromised)
        
```{r}
set.seed(5)
samp.hetero<-c("sqrt", "exp", "poly", "norm")

Hetero <- function(n, func, bool = FALSE){
  # Melissa gave us
  b0 <- 10
  b1 <- 10
  x1 <- runif(n, 0, 10)
  
  # exponential
  if (func == 'exp'){
    exp.eps <- rnorm(n, sd = exp(x1))
    exp.var<-var(exp.eps)
    exp.y <- b0 + b1*(x1) + exp.eps
    exp.ey <- b0 + b1*(x1)
    exp.lm <- lm(exp.y ~ x1)
    testing(exp.lm, exp.ey, list(b0,b1), exp.var, bool)
  }
  
  # sqrt
  if (func == 'sqrt'){
    sqrt.eps <- rnorm(n, sd = 2*sqrt(x1))
    sqrt.var<-var(sqrt.eps)
    sqrt.y <- b0 + b1*(x1) + sqrt.eps
    sqrt.ey <- b0 + b1*(x1)
    sqrt.lm <- lm(sqrt.y ~ x1)
    testing(sqrt.lm, sqrt.ey, list(b0,b1), sqrt.var, bool)
  }
  
  #poly
  if (func == 'poly'){
    poly.eps <- rnorm(n, sd = 0.5*x1^2)
    poly.var<-var(poly.eps)
    poly.y <- b0 + b1*(x1) + poly.eps
    poly.ey <- b0 + b1*(x1)
    poly.lm <- lm(poly.y ~ x1)
    testing(poly.lm, poly.ey, list(b0,b1), poly.var, bool)
  }
  
  # normal
  if(func == "norm"){
    normal(n, b0, b1, bool)
  }
}

size <- c(20,100,5000)
for (i in 1:length(samp.hetero)){
  for (j in 1:length(size)){
    print(paste("Running sample ", samp.hetero[i]," with size ",size[j]))
    Hetero(size[j],samp.hetero[i])
  }
}
```
Effect of Heterogeneous Variances  
  
Square Root  
  Small  
    - Betas are within the confidence interval   
    - Perfect EYi predictions (100%)  
    - MSE and variance are close together  
    - All coefficients are significant  
      
  Medium  
    - Betas are within the confidence interval  
    - Perfect EYi predictions (100%)  
    - MSE and variance are very close together  
    - All coefficients are significant
    
  Large  
    - Betas are within the confidence interval 
    - Perfect EYi predictions (100%)  
    - MSE and variance are nearly identical  
    - All coefficients are significant  
    
Exponential  
  Small 
  - Betas are not within the confidence interval  
  - Questionably Poor EYi prediction of (40%)  
  - MSE and variance are both very large numbers and far apart  
  - The intercept is not significant  
    
  Medium  
    - Betas are within the confidence interval
    - Perfect EYi predictions (100%)  
    - MSE and variance are both very large numbers but are fairly close together  
    - The intercept  and the x1  are not significant  
    
  Large  
    - Betas are within the confidence interval  
    - Perfect EYi predictions (100%)  
    - MSE and variance are both very large numbers but are almost identical  
    - The intercept  and the x1  are not significant  
  
Polynomial  
  Small  
  - Betas are within the confidence interval  
  - Questionable EYi prediction of (65%)  
  - MSE (503) and variance (529) are both fairly large numbers not very far apart
  - The intercept is not significant  
  
  Medium  
  - Betas are within the confidence interval  
  - Perfect EYi predictions (100%)  
  - MSE (348) and variance (351) are both fairly large numbers and are very close
  - All coefficients are significant
  
  
  Large  
  - Betas are within the confidence interval  
  - Perfect EYi predictions (100%)  
  - MSE(503) and variance (503) are both fairly large numbers and almost identical  
  - All coefficients are significant.  
  
Part e)

```{r}
set.seed(7)
n <- sample(20:5000, 1)
i <- sample(1:4, 1)
Hetero(n,samp.hetero[i],TRUE)
```
    This seems to be an example of a Large exponential heterogenous variance simulation.  This is determined by comparing the diagnostics of this simulation to the diagnostics that we calculated.  Here we can see that the EYi falls within the Yhat confidence interval 100% of the time and the MSE and variance are both very large but also almost identical.  Comparing this to our other simulations we see that the diagnostics here most closely resemble the diagnostics of a exponential heterogenous variance simulation.

\newpage

4. Correlated Errors
    a.	Simulate errors from a variety of different correlation structures.
    b.	Try simulations with a small sample size (e.g. 20), a medium sample size (e.g. n= 100), and a large sample size (e.g. n = 5000). 
    c.	For each simulation, 
        i.	Predict y-hat at several different locations using a confidence interval.
        ii.	Predict the beta coefficients for a linear model using a confidence interval. 
        iii. Find the MSE (to estimate sigma^2)
        iv. Test to see whether the beta(s) are significant (t-tests)
    d. Which of the above tasks were affected by the violation of assumptions?
    e. After you have experimented with the effects of different model structures, true parameter values, and sample sizes, let's repeat the simulation but test yourself to see whether you can detect correlated errors.
        i. Have R randomly choose whether to simulate data with correlated or uncorrelated errors.
        ii. Simulate data accordingly and display informal/ formal diagnostics as appropriate.
        iii. Based on the diagnostics, predict whether the problem areas you mentioned in part d will be affected or not. (Note: You are not predicting whether the assumptions are violated-- just whether they are violated to such an extent that your ability to use the model is compromised)

```{r}
set.seed(6)
correlated_error_options <- c("arima","rho", "normal")

correlated_error <- function(n, func, bool = FALSE){
  # Sample code to get started
  b0 <- 10
  b1 <- 10
  rho <- 0.9
  sigma <- 2
  
  x <- runif(n, 0, 10)

  # arima.sim
  if (func == "arima"){
    arima.eps <- arima.sim(model = list(ar = rho), n = n)
    arima.var <- var(arima.eps)
    arima.y <- b0 + b1 * x + arima.eps
    arima.ey <- b0 - b1*(x)
    arima.lm <- lm(arima.y ~ x)
    testing(arima.lm, arima.ey, list(b0,b1), arima.var, bool)
  }
  
  if(func == "rho"){
    rho.eps <- rep(0, n)
    e.ind <- rnorm(n, mean = 0, sd = (sigma / sqrt(1-rho^2)))
    rho.eps[1] <- e.ind[1]
    for (i in 2:n) {
      rho.eps[i] <- rho * rho.eps[i-1] + e.ind[i]
    }
    
    rho.y <- b0 + b1 * x + rho.eps
    rho.var <- var(rho.eps)
    rho.ey <- b0 - b1*(x)
    rho.lm <- lm(rho.y ~ x)
    testing(rho.lm, rho.ey, list(b0,b1), rho.var, bool)
  }
  
  # normal
  if(func == "normal"){
    normal(n, b0, b1, bool)
  }
}

for (i in 1:length(correlated_error_options)){
  for (j in 1:length(size)){
    print(paste("Running sample ", correlated_error_options[i]," with size ",size[j]))
    correlated_error(size[j], correlated_error_options[i])
  }
}
```
Effects of Correlated Error options  
Arima  
  Small  
    - Betas are within the confidence interval  
    - Worst possible EYi predictions (0%)  
    - MSE (7.6) and Variance (8.16) are close together
    - All coefficients are significant  
    
  Medium  
    - Betas are within the confidence interval  
    - Worst possible EYi predictions (0%)  
    - MSE and variance are very close together
    - All coefficients are significant
  
  Large  
    - Betas are within the confidence interval  
    - Worst possible EYi predictions (0%)  
    - MSE and variance are almost identical
    - All coefficients are significant
Rho  
  Small 
    - Betas are within the confidence interval  
    - Bad EYi predictions (5%)  
    - MSE(51) and variance(52) are close together  
    - All coefficients are significant  
  
  Medium  
    - Betas are not within the confidence interval  
    - Horrible EYi predictions (5%)  
    - MSE(45.8) and variance(45.7) are very close together  
    - The intercept is not significant  
  
  Large  
    - Betas are within the confidence interval  
    - Atrocious EYi predictions (0.36%)  
    - MSE and variance are nearly identical  
    - All coefficients are significant  

```{r}
set.seed(5)
# choose random sample size and random function
# Repeat to your hearts desire and test yourself
n <- sample(20:5000, 1)
i <- sample(1:3, 1)
print(n)
print(i)
correlated_error(n, correlated_error_options[i], TRUE)

```
    This seems to be an example of a Large Normal simulation.  This is determined by comparing the diagnostics of this simulation to the diagnostics that we calculated.  The MSE and variance are but also almost identical.  We can tell n is large by looking at the graphs, and by examining the qq plot we can see that it follows that of a normal distribution.  Comparing this to our other simulations we see that the diagnostics here most closely resemble the diagnostics of a large normal distribution.

\newpage
        
5. Multicollinearity
    a.	Simulate predictors that are correlated with a variety of different correlation structures.
    b.	Try simulations with a small sample size (e.g. 20), a medium sample size (e.g. n= 100), and a large sample size (e.g. n = 5000). 
    c.	For each simulation, 
        i.	Predict y-hat at several different locations using a confidence interval.
        ii.	Predict the beta coefficients for a linear model using a confidence interval. 
        iii. Find the MSE (to estimate sigma^2)
        iv. Test to see whether the beta(s) are significant (t-tests)
    d. Which of the above tasks were affected by the violation of assumptions?
    e. After you have experimented with the effects of different model structures, true parameter values, and sample sizes, let's repeat the simulation but test yourself to see whether you can detect collinearity.
        i. Have R randomly choose whether to simulate data with correlated or uncorrelated predictor variables (X).
        ii. Simulate data accordingly and display informal/ formal diagnostics as appropriate.
        iii. Based on the diagnostics, predict whether the problem areas you mentioned in part d will be affected or not. (Note: You are not predicting whether the assumptions are violated-- just whether they are violated to such an extent that your ability to use the model is compromised)

```{r}
set.seed(5)
multicollinearity_options <- c("linear combination","multiplier", "normal")

multicollinearity <- function(n, func, bool = FALSE){
  # Sample code to get started
  b0 <- 10
  b1 <- 3
  b2 <- 7
  sigma <- 2
  
  x1 <- runif(n, 0, 10)
  x2_adding <- x1 + rnorm(n) #x2 is a linear combination of x1 and some number
  x2_mult <-x1*rnorm(n)
  
  # linear combination
  if (func == "linear combination"){
    lc.eps <- rnorm(n, sd = sigma) 
    lc.y <- b0 + b1 * x1 + b2 * x2_adding + lc.eps
    lc.ey <- b0 + b1 * x1 + b2 * x2_adding
    lc.var <- var(lc.eps)
    lc.lm <- lm(lc.y ~ x1 + x2_adding)
    cor(x1, x2_adding)
    testing(lc.lm, lc.ey, list(b0,b1, b2), lc.var, bool)
    
  }
  
  # x2 is x1*some multiplier
  if(func == "multiplier"){
    mult.eps <- rnorm(n, sd = sigma) 
    mult.y <- b0 + b1 * x1 + b2 * x2_mult + mult.eps
    mult.ey <- b0 + b1 * x1 + b2 * x2_mult
    mult.var <- var(mult.eps)
    mult.lm <- lm(mult.y ~ x1 + x2_mult)
    testing(mult.lm, mult.ey, list(b0,b1, b2), mult.var, bool)
    cor(x1, x2_mult)
  }
  
  # normal
  if(func == "normal"){
    normal(n, b0, b1, bool)
  }
}

for (i in 1:length(multicollinearity_options)){
  for (j in 1:length(size)){
    print(paste("Running sample ", multicollinearity_options[i]," with size ",size[j]))
    multicollinearity(size[j], multicollinearity_options[i])
  }
}
```
Effects of Multicolinearity  
Linear combination  
  Small  
    - Betas are within the confidence interval  
    - Perfect EYi predictions (100%)  
    - MSE (857) and variance(2.9) are extemely far apart  
    - All coefficient are significant
    
  Medium  
    - Betas are within the confidence interval  
    - Perfect EYi predictions (100%)  
    - MSE (4351) and variance(4.6) are even further apart compared to the small model  
    - All coefficient are significant
  
  Large 
    - Betas are within the confidence interval  
    - Perfect EYi predictions (100%)  
    - MSE(250399) and variance (4) are the furthest apart
    - All coefficient are significant
  
Multiplier  
  Small  
    - Betas are within the confidence interval  
    - Perfect EYi predictions (100%)  
    - Huge difference between MSE(23988) and variance (7.19)  
    - All coefficients are significant
  
  Medium  
    - Betas are within the confidence interval  
    - Perfect EYi predictions (100%)  
    - Even larger difference between MSE(224844) and variance (3.7)than in the smaller simulation
    - All coefficients are significant
  
  Large  
    - Betas are within the confidence interval  
    - Good EYi predictions of (79.86%)  
    - Largest difference between MSE (8271184) and variance (4)
    - All coefficients are significant
  
```{r}
set.seed(5)
# choose random sample size and random function
# Repeat to your hearts desire and test yourself
n <- sample(20:5000, 1)
i <- sample(1:3, 1)
multicollinearity(n, multicollinearity_options[i], TRUE)

```
  This seems to be an example of a Large Normal simulation.  This is determined by comparing the diagnostics of this simulation to the diagnostics that we calculated.  The MSE and variance are but also almost identical.  We can tell n is large by looking at the graphs, and by examining the qq plot we can see that it follows that of a normal distribution.  Comparing this to our other simulations we see that the diagnostics here most closely resemble the diagnostics of a large normal distribution.
\newpage

6. Put it all together: Combine the code from the previous 5 parts. Have R randomly choose whether to generate data that violates one (or more) of the assumptions, or whether all the assumptions are valid. Show appropriate diagnostics and test yourself to see if you can predict whether there are problem areas or not. Repeat the simulation several times and record your accuracy at detecting the different problem areas.

```{r}
# choose random sample size and random function
# Repeat to your hearts desire and test yourself
n <- sample(20:5000, 1)
violationslist <- c("Nonlinear", "Nonnormal", "Hetero", "correlated errors", "multicollinearity")
v <- sample(1, 1)
violation <- violationslist[v]

if (violation == "Nonlinear"){
  i <- sample(1:4, 1)
  Nonlinear(n,samp[i],TRUE)
}

if (violation == "Nonnormal"){
  i <- sample(1:3, 1)
  Nonnormal(n, nonnormal_functions[i],TRUE)
}

if (violation == "Hetero"){
  i <- sample(1:4, 1)
  Hetero(n,samp.hetero[i],TRUE)
}

if (violation == "correlated errors"){
  i <- sample(1:3, 1)
  correlated_error(n, correlated_error_options[i], TRUE)
}

if (violation =="multicollinearity"){
  i <- sample(1:3, 1)
  multicollinearity(n, multicollinearity_options[i], TRUE)
}
print(v)
print(n)
```
 
  It is easy to tell the size of the data by reffering to the plots.  The QQ plot is very helpful for identifying if the data follows a normal distribution or not.  We noticed that after some time our guesses improved, it is still hard to tell the difference between the 5 different options.  However it is clear to tell whether or not a simulation is normal.  Due to the unique shape that Nonlinear simulations have on the Residuals vs Leverage graph, it was easy for us to identify these models.
  
Accuracy of predictions: 28/50 